### DeiT paper making a presentation in lab seminar
#### In this paper, they utilize 'Distillation Token' to be distilled information from teacher model.
#### Additionally, utilizing distillation classifier instead of class classifier, the model achieves higher performance.
#### Also, when using both classifiers, the model achieves the highest performance.
#### Using DeiT method, the transformer architecture achieve similar performance without external huge-scale dataset compare to SOTA CNN-based architecture.
